---
title: "Unidad 5 — Agrupando Observaciones"
subtitle: "Semana 09: Clustering jerárquico"
author: "Eduard F. Martínez González"
---
  
<a href="mailto:efmartinez@icesi.edu.co" style="color:black;">
<img src="pic/correo.png" alt="Email" width="20" height="20"/> efmartinez@icesi.edu.co
</a>

<a href="https://github.com/eduard-martinez" style="color:black;"> <img src="pic/github.png" alt="Qries" width="20" height="20"/> eduard-martinez</a>

<a href="https://twitter.com/emartigo" style="color:black;"> <img src="pic/twitter.jpg" alt="Qries" width="20" height="20"/> @emartigo</a>

<a href="https://eduard-martinez.github.io" style="color:black;"> <img src="pic/link.png" alt="Qries" width="20" height="20"/> https://eduard-martinez.github.io</a>

---

# Clustering jerárquico y dendrogramas

El clustering jerárquico construye una estructura en forma de árbol que muestra cómo se agrupan progresivamente las observaciones según su semejanza. La representación visual es el dendrograma: cada “hoja” corresponde a una observación; las “ramas” reflejan fusiones de grupos; y la altura de unión indica la diferencia (distancia) entre los grupos que se combinan.

## Enfoques aglomerativo y divisivo

Existen dos enfoques conceptuales. En el aglomerativo (bottom-up), se inicia con cada observación como su propio grupo y, en pasos sucesivos, se fusionan los clusters más próximos hasta obtener una sola agrupación. En el divisivo (top-down), se parte de un único gran cluster y se divide recursivamente. En la práctica docente suele emplearse principalmente el enfoque aglomerativo por su interpretabilidad y disponibilidad en paquetes estándar.

## Proximidad: métricas y criterios de enlace

La noción de “proximidad” depende de dos decisiones. Primero, la métrica de distancia; con variables en escalas distintas, se recomienda estandarizar (media 0, desviación 1) y usar, por ejemplo, distancia euclidiana. Segundo, el criterio de enlace (linkage), que define la distancia entre clusters: single (distancia mínima entre pares, propenso a “encadenar”), complete (distancia máxima, favorece clusters compactos), average (promedio de distancias) y Ward.D2 (fusiona minimizando el aumento de varianza intracluster, produciendo grupos usualmente más esféricos y balanceados). A diferencia de K-means, no se fija el número de clusters k de entrada; el dendrograma permite seleccionar k ex-post.

## Elección del número de clusters

El número de grupos se elige “cortando” horizontalmente el dendrograma a una cierta altura. Un corte alto produce menos grupos y mayor generalidad; un corte bajo produce más grupos y mayor especificidad. Saltos grandes en la altura de las fusiones sugieren fronteras naturales entre segmentos y orientan una elección prudente de k.

## Algoritmo aglomerativo: descripción operativa

Para implementar el enfoque aglomerativo, se procede usualmente de la siguiente manera. Primero, se preparan las variables; cuando se encuentran en escalas heterogéneas, se estandarizan. Luego, se calcula la matriz de distancias entre observaciones con la métrica elegida. A continuación, se inicia con n clusters (uno por observación) y, de forma iterativa, se identifican y fusionan los dos clusters más cercanos según el criterio de enlace seleccionado, actualizando después las distancias entre el nuevo cluster y los existentes. El proceso continúa hasta que queda un único cluster; el historial de fusiones define el dendrograma. Finalmente, se selecciona k cortando el dendrograma a una altura que evite fusiones “costosas”, es decir, asociadas a incrementos bruscos en la altura.

## Lectura rápida de un dendrograma

La interpretación sigue pautas simples. Las hojas representan observaciones individuales. La altura de unión indica cuán diferentes eran los grupos al fusionarse: una unión a gran altura sugiere que los grupos combinados estaban muy separados. Un corte horizontal determina el número de clusters; las ramas que permanecen separadas por debajo del corte definen los grupos resultantes. La presencia de grandes saltos verticales antes de ciertas fusiones suele señalar límites naturales entre segmentos.

## Ventajas y limitaciones

Entre las ventajas, no se requiere fijar k al inicio, se obtiene una representación jerárquica que facilita justificar segmentaciones y se dispone de una lectura multiescala de la estructura de los datos. Entre las limitaciones, su complejidad computacional puede ser mayor que la de K-means en conjuntos de datos muy grandes, y la solución es sensible tanto a las escalas de las variables como a la elección de métrica y criterio de enlace, por lo que la estandarización y la verificación de robustez resultan recomendables.

---

# Aplicación en R

Para ejemplificar el uso de `dendrogramas` se trabajará con una base de notas del curso, que reúne para cada estudiante sus calificaciones en talleres, quices, parcial y midterm. Imagine un diagrama de dispersión donde en el eje X se ubica el promedio de talleres y en el eje Y el promedio de quices. Al ajustar K-means con k = 3, el algoritmo puede revelar patrones como los siguientes: Cluster 1, formado por estudiantes con desempeños altos y consistentes en trabajos y evaluaciones cortas; Cluster 2, con estudiantes que rinden mejor en actividades prácticas (talleres) pero muestran rezagos en quices; y Cluster 3, que agrupa desempeños más bajos o inestables en ambas dimensiones.

Este ejercicio permite observar cómo el algoritmo construye segmentos naturales a partir de los datos, sin categorías predefinidas, resaltando la utilidad del aprendizaje no supervisado para explorar perfiles y patrones de desempeño académico.

::: callout-tip
**Cómo usar este material:** Puedes ejecutar los _chunks_ de R directamente en el navegador gracias a **webR** (según tu `_quarto.yml`), sin instalar nada localmente.
:::

## Preparación del entorno

El propósito de este bloque es asegurar un entorno limpio y reproducible. Primero, eliminamos objetos previos que puedan interferir con el análisis. Después, cargamos los paquetes necesarios para manipulación de datos, visualización, generación de resúmenes y ejecución de un `dendrograma`. Con esto, dejamos el entorno preparado para comenzar.

```{webr-r}
## Author:
## Date: 

## limpiar entorno
rm(list = ls())

## cargar paquetes
require(dplyr)      # Manipulación de datos 
require(ggplot2)    # Visualización de datos
require(skimr)      # Resúmenes rápidos y completos

## print
print("Listo!")
```

## Ingesta de datos (carga desde archivo o URL)

En esta sección se realiza la ingesta de datos, es decir, la carga de la base de notas del curso que usaremos para los ejercicios de clustering. El dataset llamado `grades` incluye, para cada estudiante, su código y las calificaciones en `talleres`, `quices`, `parcial` y `midterm` (escala 0–5). Estos atributos permitirán aplicar `K-means` para identificar patrones de desempeño y segmentar a los estudiantes en grupos con perfiles similares.

Para garantizar reproducibilidad, podemos trabajar de dos maneras:  

1. **Archivo local**: leer directamente el archivo `data.csv` que acompaña a este material.  
2. **Fuente externa (opcional)**: usar una URL (por ejemplo, un repositorio en GitHub) para que cualquier estudiante pueda replicar el ejercicio sin necesidad del archivo local.

Una vez cargados los datos, listamos los objetos en memoria para confirmar que el dataset esperado (`grades`) está disponible y listo para la exploración inicial.

```{webr-r}
#| warning: false
#| message: false

## generar los datos
source("https://raw.githubusercontent.com/ba-in-r/01-slides/main/week-09/data/grades.r")

## chuequear objetos en la memoria
ls()

## atributos de grades
class(grades)
skim(grades)
```

## Selección de variables

En este paso se eligen únicamente las variables numéricas relevantes para el agrupamiento. En el caso de las notas, pueden incluirse `taller`, `quiz`, `parcial` y `midterm.` Estas variables son adecuadas para `dendrograma` porque están en la misma escala (0–5) y describen cuantitativamente el desempeño académico.

Para un ejemplo simple, trabajaremos con dos dimensiones (`quiz` y `midterm`) a fin de visualizar con claridad la lógica del clustering. Al reducir la base a este subconjunto se garantiza que el análisis se centre en atributos comparables y se excluyan identificadores u otras columnas que no aportan a la formación de grupos.

```{webr-r}
## filtrar variables
db <- select(.data=grades , midterm,  quiz , group)

## print
print("Listo!")
```

## Escalado

Antes de aplicar `dendrograma` es buena práctica escalar las variables para que todas queden con media 0 y desviación estándar 1. Así se evita que alguna dimensión pese más en el cálculo de distancias euclidianas. Aunque aquí todas las notas comparten el rango 0–5, el escalado ayuda a estandarizar y comparar contribuciones.

Con `scale()` se obtiene una matriz estandarizada lista para el análisis. Luego, se usa `skim()` para comprobar rápidamente que la transformación se aplicó correctamente (medias cercanas a 0 y desviaciones cercanas a 1).

```{webr-r}
## escalar
db <- scale(db)

## check data (skim)
skim(db)

## visualicemos las variables
ggplot(data=db , aes(x=midterm , y=quiz)) +
geom_point() +
theme_minimal()
```

## Distancias

**¿Qué hace este bloque y qué devuelve?** Matriz de distancias. A partir del conjunto estandarizado `db`, se construye la matriz de distancias. Para `Ward.D2`, utiliza `method = "euclidean"`.

```{webr-r}
## escalar
D  <- dist(db , method="euclidean")

## print
head(D)
```

### H-Cluster

**`ward.D2`**: fusiona los grupos que **menor incremento** generan en la suma de cuadrados intra‑cluster (varianza). Suele producir **clusters compactos** y de tamaño similar. Recomendado con **distancia euclidiana** y **datos escalados**.

**`complete`**: distancia entre grupos = **máxima distancia** entre sus puntos. Tiende a formar grupos **apretados** y separa bien **outliers**, pero puede ser **sensible a ruido extremo**.

**Idea práctica**: Entrenamos dos variantes para comparar la estructura del árbol: `Ward.D2` y `Complete`.

```{webr-r}
## hclust ward
hc_ward     <- hclust(D, method = "ward.D2")

## print
print("Listo!")
```
Componentes clave de un objeto `hclust`

- `merge`: matriz de tamaño `(n - 1) × 2` que registra qué grupos se fusionan en cada paso.
  - Índices **negativos** (`-i`) son observaciones individuales (hojas).
  - Índices **positivos** (`j`) refieren a clusters formados en el **paso `j`**.
- `height`: vector con la disimilitud (altura) a la que ocurre cada fusión.
- `order`: orden de las hojas (útil al graficar).
- `labels`: etiquetas de las observaciones (si estaban disponibles).
- `method`: método de enlace utilizado.
- `call`, `dist.method`: metadatos de la llamada y de la distancia.

```{webr-r}
## Resumen básico
print(hc_ward)

## Tipo y componentes
class(hc_ward)
names(hc_ward)
str(hc_ward, max.level = 1)

## Detalle de fusiones y alturas
dim(hc_ward$merge)
head(hc_ward$merge)
head(hc_ward$height)
head(hc_ward$order)

## Método de enlace y distancia (si está disponible)
hc_ward$method
hc_ward$dist.method
```

## Dendrogramas

Un dendrograma representa la historia de fusiones del clustering jerárquico: 

+ La altura de fusión (eje vertical) aproxima la disimilitud al unir dos ramas: más alto ⇒ más diferentes eran los grupos. 
+ Un corte horizontal a una altura fija particiona el árbol en k clusters. 
+ Ramas largas que se unen tarde pueden señalar outliers o grupos bien separados. 
+ El orden de las hojas (eje horizontal) es conveniente para lectura, pero no tiene escala métrica.

```{webr-r}
par(mfrow = c(1,2))
plot(hc_ward, labels = grades$codigo, main = "Dendrograma - Ward.D2", xlab = "", sub = "")
rect.hclust(hc_ward, k = 3, border = "red")
```

## Corte del árbol y comparación con group

El bloque corta el árbol jerárquico obtenido con hc_ward en k = 3 grupos usando cutree(), creando así la asignación de cada estudiante a un cluster. Luego agrega esa etiqueta (cluster_w) a la tabla original de notas (grades) y la convierte en factor para facilitar análisis y gráficos. Con count() calcula el tamaño de cada cluster, lo que permite ver la distribución de estudiantes por grupo (en el ejemplo: 18, 18 y 35).

Después, con un group_by(cluster_w) y summarise(across(...)), calcula los perfiles promedio por cluster en las cuatro evaluaciones (taller, quiz, parcial, midterm), ignorando faltantes con na.rm = TRUE. Los resultados impresos ilustran cómo interpretar esos perfiles: p. ej., el cluster 1 muestra alto desempeño en talleres (4.48) y quices (4.84) pero menor en parcial (2.98); el cluster 2 refleja un rezago general; y el cluster 3 combina buen rendimiento práctico (talleres 4.54) con niveles intermedios en quiz y parcial. Con esto, se obtiene tanto la composición de los clusters como su caracterización académica.

```{webr-r}
## crear clusters
k <- 3
cl_w <- cutree(hc_ward, k = k)

# Añadir cluster a la tabla original
grades_w <- grades %>% mutate(cluster_w = factor(cl_w))

# Tamaños por cluster
count(grades_w, cluster_w)

# Perfiles (medias por nota)
profiles_w <- grades_w %>%
              group_by(cluster_w) %>%
              summarise(across(c(taller, quiz, parcial, midterm), ~ mean(.x, na.rm = TRUE)),
                        .groups = "drop")
profiles_w
```

